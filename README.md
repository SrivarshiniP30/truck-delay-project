# Truck Delay Classification and ETA Prediction

## ğŸš€ Project Overview: An End-to-End Machine Learning Solution

This project presents a comprehensive, end-to-end machine learning pipeline designed to predict truck delays and estimate arrival times (ETA). It covers the entire lifecycle of a real-world machine learning application, from initial data handling to model deployment and potential fine-tuning.

The goal is to analyze various factors influencing truck delaysâ€”such as weather conditions, traffic patterns, and driver behaviorâ€”and leverage this understanding to build robust predictive models. This project demonstrates proficiency in:

* **Data Engineering:** From raw data ingestion, through meticulous cleaning, preprocessing, and feature engineering.
* **Exploratory Data Analysis (EDA):** Deep-diving into data to uncover insights and patterns that drive model development.
* **Machine Learning Model Development:** Training and evaluating multiple algorithms, including Logistic Regression, Random Forest, and XGBoost, with a focus on performance and interpretability.
* **MLOps Practices:** Utilizing MLflow for experiment tracking, model versioning, and managing the machine learning lifecycle.
* **Deployment Readiness:** Structuring the project for easy integration into an application, demonstrating readiness for production environments and continuous fine-tuning.

This solution ultimately aims to **optimize delivery schedules, minimize operational costs, and enhance customer satisfaction** by accurately predicting and mitigating delays.

## ğŸŒŸ Key Features

* **Full ML Lifecycle Implementation:** Demonstrates a complete machine learning workflow from raw data to deployable models.
* **Data-Driven Insights:** **Performs comprehensive analysis** to identify key drivers of delay, **informing robust feature selection**.
* **Advanced Predictive Modeling:** **Develops and rigorously evaluates** various supervised learning algorithms for delay classification and ETA prediction.
* **MLOps Integration with MLflow:** Ensures reproducibility, version control for models, and efficient experiment management.
* **Modular & Scalable Design:** Project structured with reusable components and pipelines for maintainability and future expansion.
* **Data Management:** Clear separation of raw and processed data for improved data governance and pipeline clarity.

## ğŸ› ï¸ Technologies Used

* **Programming Languages:** Python
* **Machine Learning Libraries:** Scikit-learn, XGBoost
* **Data Manipulation:** Pandas, NumPy
* **Notebooks:** Jupyter Notebook
* **MLOps:** MLflow
* **Database (Mock/Example):** PostgreSQL (referenced in `truck-eta-postgres.sql`)

## ğŸ“ Project Structure

    .
    â”œâ”€â”€ app/                          # Application code for deployment/serving the model
    â”‚   â”œâ”€â”€ app.py                    # Main application entry point
    â”‚   â””â”€â”€ modelDeployed.py          # Script for loading and serving the deployed model
    â”œâ”€â”€ config/                       # Configuration files for the project
    â”‚   â””â”€â”€ config.ini
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ raw/                      # Original, untouched raw data
    â”‚   â”‚   â”œâ”€â”€ city_weather.csv
    â”‚   â”‚   â”œâ”€â”€ drivers_table.csv
    â”‚   â”‚   â”œâ”€â”€ routes_table.csv
    â”‚   â”‚   â”œâ”€â”€ routes_weather.csv
    â”‚   â”‚   â”œâ”€â”€ traffic_table.csv
    â”‚   â”‚   â”œâ”€â”€ truck_schedule_table.csv
    â”‚   â”‚   â””â”€â”€ trucks_table.csv
    â”‚   â”‚   â””â”€â”€ truck-eta-postgres.sql
    â”‚   â””â”€â”€ processed/                # Cleaned, transformed, or feature-engineered data
    â”‚       â”œâ”€â”€ cleanedDf.csv
    â”‚       â”œâ”€â”€ combineddf.csv
    â”‚       â”œâ”€â”€ final_data.csv        # Final processed data (LFS tracked)
    â”‚       â”œâ”€â”€ final_data2.csv
    â”‚       â”œâ”€â”€ X_test.csv
    â”‚       â””â”€â”€ y_test.csv
    â”œâ”€â”€ models/                       # Trained model artifacts and related metadata
    â”‚   â”œâ”€â”€ best_model.pkl            # Best performing trained machine learning model (LFS tracked)
    â”‚   â””â”€â”€ best_params.json          # Best parameters found during hyperparameter tuning
    â”œâ”€â”€ mlruns/                 # MLflow experiment tracking data and artifacts
    â”œâ”€â”€ notebooks/                    # Jupyter notebooks for EDA, experimentation, and analysis
    â”‚   â”œâ”€â”€ Data_Cleaning_Preprocessing.ipynb
    â”‚   â”œâ”€â”€ Data_Collection_Ingestion.ipynb
    â”‚   â”œâ”€â”€ EDA_Feature_Engineering.ipynb
    â”‚   â”œâ”€â”€ EDA_Final.ipynb
    â”‚   â”œâ”€â”€ EDA_Initial.ipynb
    â”‚   â”œâ”€â”€ MLflow_Experiment_1.ipynb
    â”‚   â”œâ”€â”€ MLflow_Experiment_2.ipynb
    â”‚   â”œâ”€â”€ Model_Development_Exploration.ipynb
    â”‚   â”œâ”€â”€ Model_Evaluation.ipynb
    â”‚   â”œâ”€â”€ Model_Training_LogReg.ipynb
    â”‚   â”œâ”€â”€ Model_Training_LogReg_2.ipynb
    â”‚   â”œâ”€â”€ Model_Training_RandomForest.ipynb
    â”‚   â”œâ”€â”€ Model_Training_RandomForest_2.ipynb
    â”‚   â”œâ”€â”€ Model_Training_XGBoost.ipynb
    â”‚   â”œâ”€â”€ Model_Training_XGBoost_2.ipynb
    â”‚   â””â”€â”€ old_notebooks/            # Older or redundant notebooks for reference
    â”‚       â”œâ”€â”€ dataCleaning(WRONG).ipynb
    â”œâ”€â”€ src/                          # Reusable Python source code
    â”‚   â”œâ”€â”€ components/               # Modular components for data processing, feature engineering, etc.
    â”‚   â”‚   â”œâ”€â”€ init.py
    â”‚   â”‚   â”œâ”€â”€ data_cleaning.py
    â”‚   â”‚   â””â”€â”€ data_ingestion.py
    â”‚   â”œâ”€â”€ pipelines/                # Scripts that orchestrate the data flow and ML steps
    â”‚   â”‚   â”œâ”€â”€ init.py
    â”‚   â”‚   â”œâ”€â”€ data_cleaning_pipeline.py
    â”‚   â”‚   â””â”€â”€ data_ingestion_pipeline.py
    â”‚   â”œâ”€â”€ models/                   # Python scripts for model definition, training, and prediction logic
    â”‚   â”‚   â”œâ”€â”€ init.py
    â”‚   â”‚   â”œâ”€â”€ model_training.py     # (Placeholder for combined model training logic)
    â”‚   â”‚   â””â”€â”€ prediction.py         # (Placeholder for inference logic)
    â”‚   â””â”€â”€ utils/                    # Utility functions (e.g., for data loading, common helpers)
    â”‚       â”œâ”€â”€ init.py
    â”‚       â”œâ”€â”€ data_loader.py
    â”‚       â””â”€â”€ mlflow_utils.py       # (Placeholder for MLflow utility functions)
    â”œâ”€â”€ .gitattributes                # Git LFS tracking configuration
    â”œâ”€â”€ .gitignore                    # Files/folders to ignore from Git
    â”œâ”€â”€ README.md                     # Project overview
    â”œâ”€â”€ requirements.txt              # Python dependencies
    â”œâ”€â”€ setup.py                      # Project setup script
    â”œâ”€â”€ data.json                     # Configuration/data metadata
    â”œâ”€â”€ feature_names.json
    â””â”€â”€ logRegMlFlow.py               # (Standalone MLflow script, consider integrating into notebooks/src)



## ğŸš€ Setup and Installation

To set up and run this project locally, follow these steps:

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/SrivarshiniP30/truck-delay-project.git](https://github.com/SrivarshiniP30/truck-delay-project.git)
    cd truck-delay-project
    ```
2.  **Install Git LFS:**
    Ensure Git Large File Storage (LFS) is installed on your system to handle large data and model files.
    ```bash
    git lfs install
    ```
3.  **Pull LFS files:**
    ```bash
    git lfs pull
    ```
4.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    # On Windows:
    .\venv\Scripts\activate
    # On macOS/Linux:
    source venv/bin/activate
    ```
5.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *(Note: Ensure your `requirements.txt` is up-to-date with all project dependencies. If missing, you may need to manually install `pandas`, `numpy`, `scikit-learn`, `xgboost`, `mlflow`, `jupyter`.)*

## ğŸ“Š Usage

This project follows a structured data science workflow. Key stages are demonstrated through the Jupyter Notebooks and Python scripts:

* **Data Collection & Ingestion:** Refer to `notebooks/Data_Collection_Ingestion.ipynb` and `src/utils/data_loader.py`.

* **Data Cleaning & Preprocessing:** Explore `notebooks/Data_Cleaning_Preprocessing.ipynb` and `src/components/data_cleaning.py`.

* **EDA & Feature Engineering:** See `notebooks/EDA_Feature_Engineering.ipynb`, `notebooks/EDA_Initial.ipynb`, and `notebooks/EDA_Final.ipynb`.

* **Model Training & Evaluation:** Check notebooks like `notebooks/Model_Training_RandomForest.ipynb`, `notebooks/Model_Training_XGBoost.ipynb`, `notebooks/Model_Training_LogReg.ipynb` for different model approaches.

* **MLflow Tracking:** `notebooks/MLflow_Experiment_1.ipynb`, `notebooks/MLflow_Experiment_2.ipynb`, and `logRegMlFlow.py` (consider moving to `src/utils/mlflow_utils.py` or integrating into notebooks) demonstrate MLflow integration.

* **Model Deployment/Inference:** `app/app.py` and `app/modelDeployed.py` are used for serving the trained model.

To run the Jupyter Notebooks:
```bash
jupyter notebook



